{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8eb6113-1e29-446f-9bd8-8b98adc8a7f2",
   "metadata": {},
   "source": [
    "# Kalman filter for state estimation and occlusion percentage\n",
    "We get in input the output of the yolo, so a set of bounding box, defined with their center (x,y) coordinates\n",
    "and the width and the height of this rectangle.\n",
    "By evaluating the percentage of the area visible of each bounding box, we can say if a pedestrian/cyclist/car is occluded, and if it is so, we'll call the state estimation function to predict its movement under the occlusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14719b6c-8236-4c10-940b-b0b3914ae531",
   "metadata": {},
   "source": [
    "## YOLOv5 Detection Results for Image: x512.png\n",
    "Format: [class] [confidence_score] [x_center] [y_center] [width] [height]\n",
    "\n",
    "Vehicles\n",
    "vehicle 0.94 0.54 0.65 0.10 0.15;\n",
    "vehicle 0.93 0.48 0.63 0.10 0.15;\n",
    "vehicle 0.91 0.50 0.60 0.10 0.15;\n",
    "vehicle 0.88 0.45 0.58 0.10 0.15;\n",
    "vehicle 0.84 0.40 0.55 0.10 0.15;\n",
    "vehicle 0.70 0.38 0.53 0.10 0.15;\n",
    "\n",
    "Pedestrians\n",
    "pedestrian 0.88 0.57 0.68 0.05 0.10;\n",
    "pedestrian 0.80 0.59 0.70 0.05 0.10;\n",
    "pedestrian 0.76 0.52 0.69 0.05 0.10;\n",
    "pedestrian 0.75 0.60 0.64 0.05 0.10;\n",
    "pedestrian 0.71 0.61 0.66 0.05 0.10;\n",
    "pedestrian 0.67 0.62 0.62 0.05 0.10;\n",
    "pedestrian 0.66 0.63 0.61 0.05 0.10;\n",
    "\n",
    "# Occlusion rate\n",
    "Performance: This approach has \n",
    "O(n2) complexity due to the nested loop for each pair of bounding boxes. For large numbers of objects,we may want to optimize it by using spatial data structures like a KD-tree.\n",
    "Partial Occlusion: This method doesnâ€™t account for partial occlusion from objects with irregular shapes. It assumes that occlusions are always rectangular.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97363423-7318-499c-996d-ecabe50bdc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Calculate occlusion rates\\nocclusion_rates = []\\nfor i, (label_i, x1_i, y1_i, x2_i, y2_i) in enumerate(bounding_boxes):\\n    box_i_area = (x2_i - x1_i) * (y2_i - y1_i)\\n    occluded_area = 0\\n    \\n    # Compare with all other boxes\\n    for j, (label_j, x1_j, y1_j, x2_j, y2_j) in enumerate(bounding_boxes):\\n        if i != j:  # Skip self-comparison\\n            occlusion_area = calculate_occlusion_area((x1_i, y1_i, x2_i, y2_i), (x1_j, y1_j, x2_j, y2_j))\\n            occluded_area += intersection_area\\n            # sometimes oclluded_area > 100% because same parts of the analyzed bounding box are\\n            # occluded by more than 1 bounding box.\\n    \\n    # Calculate occlusion rate as a percentage\\n    occlusion_rate = (occluded_area / box_i_area) * 100 if box_i_area > 0 else 0\\n    occlusion_rates.append((label_i, occlusion_rate))\\n    \\n    # Display occlusion rate\\n    print(f\"Label: {label_i}, Occlusion Rate: {occlusion_rate:.2f}%\")\\n\\n# Display the final image\\ncv2.imshow(\"Occlusion Rates\", image)\\ncv2.waitKey(0)\\ncv2.destroyAllWindows()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate intersection area between a box and the overlay region\n",
    "def calculate_occlusion_area(box, overlay_rect):\n",
    "    x1 = max(box[0], overlay_rect[0])\n",
    "    y1 = max(box[1], overlay_rect[1])\n",
    "    x2 = min(box[2], overlay_rect[2])\n",
    "    y2 = min(box[3], overlay_rect[3])\n",
    "    \n",
    "    intersection_width = max(0, x2 - x1)\n",
    "    intersection_height = max(0, y2 - y1)\n",
    "    return intersection_width * intersection_height\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(\"urban-detection-master/res/s512.png\")\n",
    "\n",
    "# Path to YOLO output file\n",
    "file_path = \"yolo_output.txt\" \n",
    "\n",
    "# Image dimensions for denormalization\n",
    "img_height, img_width, _ = image.shape\n",
    "\n",
    "# Parse bounding boxes from the file\n",
    "bounding_boxes = []\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        if line.startswith(\"#\") or line.strip() == \"\":\n",
    "            continue\n",
    "        \n",
    "        parts = line.strip().split()\n",
    "        label = parts[0]\n",
    "        confidence = float(parts[1])\n",
    "        x_center = float(parts[2])\n",
    "        y_center = float(parts[3])\n",
    "        width = float(parts[4])\n",
    "        height = float(parts[5])\n",
    "        \n",
    "        # Denormalize coordinates\n",
    "        x_center = int(x_center * img_width)\n",
    "        y_center = int(y_center * img_height)\n",
    "        box_width = int(width * img_width)\n",
    "        box_height = int(height * img_height)\n",
    "        \n",
    "        # Calculate top-left and bottom-right corners of the bounding box\n",
    "        x1 = int(x_center - box_width / 2)\n",
    "        y1 = int(y_center - box_height / 2)\n",
    "        x2 = int(x_center + box_width / 2)\n",
    "        y2 = int(y_center + box_height / 2)\n",
    "        \n",
    "        # Append the bounding box with label and confidence\n",
    "        bounding_boxes.append((label, x1, y1, x2, y2,x_center,y_center))\n",
    "\n",
    "'''\n",
    "# Calculate occlusion rates\n",
    "occlusion_rates = []\n",
    "for i, (label_i, x1_i, y1_i, x2_i, y2_i) in enumerate(bounding_boxes):\n",
    "    box_i_area = (x2_i - x1_i) * (y2_i - y1_i)\n",
    "    occluded_area = 0\n",
    "    \n",
    "    # Compare with all other boxes\n",
    "    for j, (label_j, x1_j, y1_j, x2_j, y2_j) in enumerate(bounding_boxes):\n",
    "        if i != j:  # Skip self-comparison\n",
    "            occlusion_area = calculate_occlusion_area((x1_i, y1_i, x2_i, y2_i), (x1_j, y1_j, x2_j, y2_j))\n",
    "            occluded_area += intersection_area\n",
    "            # sometimes oclluded_area > 100% because same parts of the analyzed bounding box are\n",
    "            # occluded by more than 1 bounding box.\n",
    "    \n",
    "    # Calculate occlusion rate as a percentage\n",
    "    occlusion_rate = (occluded_area / box_i_area) * 100 if box_i_area > 0 else 0\n",
    "    occlusion_rates.append((label_i, occlusion_rate))\n",
    "    \n",
    "    # Display occlusion rate\n",
    "    print(f\"Label: {label_i}, Occlusion Rate: {occlusion_rate:.2f}%\")\n",
    "\n",
    "# Display the final image\n",
    "cv2.imshow(\"Occlusion Rates\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a31ea6-6cec-4411-89ae-bf98d673f013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: vehicle, Confidence: 0.41, Occlusion Rate: 11.85%\n",
      "Label: vehicle, Confidence: 0.41, Occlusion Rate: 51.30%\n",
      "Label: vehicle, Confidence: 0.41, Occlusion Rate: 51.56%\n",
      "Label: vehicle, Confidence: 0.41, Occlusion Rate: 93.59%\n",
      "Label: vehicle, Confidence: 0.41, Occlusion Rate: 100.00%\n",
      "Label: vehicle, Confidence: 0.41, Occlusion Rate: 100.00%\n",
      "Label: pedestrian, Confidence: 0.41, Occlusion Rate: 0.00%\n",
      "Label: pedestrian, Confidence: 0.41, Occlusion Rate: 0.00%\n",
      "Label: pedestrian, Confidence: 0.41, Occlusion Rate: 2.49%\n",
      "Label: pedestrian, Confidence: 0.41, Occlusion Rate: 0.00%\n",
      "Label: pedestrian, Confidence: 0.41, Occlusion Rate: 0.00%\n",
      "Label: pedestrian, Confidence: 0.41, Occlusion Rate: 0.00%\n",
      "Label: pedestrian, Confidence: 0.41, Occlusion Rate: 0.00%\n",
      "Label: construction, Confidence: 0.41, Occlusion Rate: 84.82%\n",
      "[('vehicle', 366, 299, 440, 377, 403, 338, 2), ('vehicle', 322, 288, 396, 366, 359, 327, 1), ('vehicle', 322, 288, 396, 366, 359, 327, 3), ('vehicle', 337, 273, 411, 351, 374, 312, 1), ('vehicle', 337, 273, 411, 351, 374, 312, 3), ('vehicle', 299, 262, 373, 340, 336, 301, 1), ('vehicle', 299, 262, 373, 340, 336, 301, 3), ('vehicle', 262, 247, 336, 325, 299, 286, 0), ('vehicle', 262, 247, 336, 325, 299, 286, 3), ('vehicle', 247, 236, 321, 314, 284, 275, 0), ('vehicle', 247, 236, 321, 314, 284, 275, 3), ('pedestrian', 407, 327, 444, 379, 426, 353, 2), ('pedestrian', 422, 338, 459, 390, 441, 364, 2), ('pedestrian', 369, 332, 406, 384, 388, 358, 2), ('pedestrian', 429, 306, 466, 358, 448, 332, 2), ('pedestrian', 437, 317, 474, 369, 456, 343, 2), ('pedestrian', 444, 296, 481, 348, 463, 322, 2), ('pedestrian', 452, 291, 489, 343, 471, 317, 2), ('construction', 183, 244, 295, 296, 239, 270, 1), ('construction', 183, 244, 295, 296, 239, 270, 3)]\n"
     ]
    }
   ],
   "source": [
    "# let's consider the overlap from an entire image (Wazowzky)\n",
    "overlap_image = cv2.imread(\"Overlap_image.png\")\n",
    "# Define overlay image position and size on the original image\n",
    "overlay_x, overlay_y = 200, 150  # Top-left corner of the overlay on the original image (assumed)\n",
    "overlap_image = cv2.resize(overlap_image,(round(overlap_image.shape[1]/3), round(overlap_image.shape[0]/3)))\n",
    "overlay_width, overlay_height = overlap_image.shape[1], overlap_image.shape[0]\n",
    "\n",
    "# Define overlay region as a bounding box\n",
    "overlay_rect = (overlay_x, overlay_y, overlay_x + overlay_width, overlay_y + overlay_height)\n",
    "\n",
    "# Calculate occlusion rate for each bounding box\n",
    "occlusion_rates = []\n",
    "for label, x1, y1, x2, y2, x_center, y_center in bounding_boxes:\n",
    "    box_area = (x2 - x1) * (y2 - y1)\n",
    "    occlusion_area = calculate_occlusion_area((x1, y1, x2, y2), overlay_rect)\n",
    "    \n",
    "    occlusion_rate = (occlusion_area / box_area) * 100 if box_area > 0 else 0\n",
    "    \n",
    "    if occlusion_rate == 100:\n",
    "        occlusion_rates.append((label, x1, y1, x2, y2, x_center, y_center, 0)) # not visible at all\n",
    "    if occlusion_rate<100 and occlusion_rate >= 50:\n",
    "        occlusion_rates.append((label, x1, y1, x2, y2, x_center, y_center, 1)) # partially visible\n",
    "    if occlusion_rate<50 and occlusion_rate >= 0:\n",
    "        occlusion_rates.append((label, x1, y1, x2, y2, x_center, y_center, 2)) # mostly visible\n",
    "    else:\n",
    "        occlusion_rates.append((label, x1, y1, x2, y2, x_center, y_center, 3)) # totally visible\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Label: {label}, Confidence: {confidence:.2f}, Occlusion Rate: {occlusion_rate:.2f}%\")\n",
    "\n",
    "'''# display the overlay region on the original image\n",
    "cv2.rectangle(image, (overlay_x, overlay_y), (overlay_x + overlay_width, overlay_y + overlay_height), (0, 0, 255), 2)\n",
    "for label, x1, y1, x2, y2 in bounding_boxes:\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    cv2.putText(image, f\"{label} \", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)'''\n",
    "\n",
    "print(occlusion_rates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdd3514-5840-47de-8ea5-fa7bd9420fff",
   "metadata": {},
   "source": [
    "# Kalman filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c20a7bd0-4d52-49b4-8732-6fb5a1852eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load the video\\ncap = cv2.VideoCapture(\\'rolling_ball.mp4\\')\\nif not cap.isOpened():\\n    print(\"Cannot open video\")\\n    exit()\\n\\n# Sharpening kernel\\n# to emphasize the central pixels\\nsharpen_kernel = np.array([[-1, -1, -1],\\n                           [-1,  9, -1],\\n                           [-1, -1, -1]])\\n\\n# Initialize radius with a default value\\nradius = 0\\n\\n# Looping through all the frames\\nwhile True:\\n    ret, frame = cap.read()\\n    if not ret:\\n        break\\n        \\n    ### Detect the ball ###\\n    # Filter by red color first\\n    red_frame = filter_red_color(frame)\\n\\n    # Convert the red frame to grayscale for edge detection\\n    gray = cv2.cvtColor(red_frame, cv2.COLOR_BGR2GRAY)\\n\\n    # Apply a Gaussian blur\\n    gray_blurred = cv2.GaussianBlur(gray, (5, 5), 0)\\n\\n    # Detect edges\\n    edges = cv2.Canny(gray_blurred, 30, 50)\\n\\n    # Apply HoughCircles to detect circles\\n    pieces = cv2.HoughCircles(edges, cv2.HOUGH_GRADIENT, 1, 80, param1=100, param2=20, minRadius=40, maxRadius=125)\\n\\n    # We now display the ball if it was found\\n    if pieces is not None:\\n        # Convert the (x, y, radius) values of the circles to integers\\n        pieces = np.uint16(np.around(pieces))\\n    \\n        # Draw the circles on the original image\\n        circle = pieces[0, 0]\\n        center = (circle[0], circle[1])\\n        radius = circle[2]\\n\\n        cv2.circle(frame, center, radius, (0, 255, 0), 5)  # Green circle with thickness 2\\n    \\n        ### If the ball is found, update the Kalman filter ###\\n        Z = np.array(center)  # Z holds the \"measured position\" of the ball\\n        x, P = update(x, P, Z, H, R)\\n    \\n    ### Predict the next state\\n    x, P = predict(x, P, F, u)\\n    predicted_center = (int(x[0]), int(x[3]))\\n    ### Draw the predicted state on the image frame ###\\n    cv2.circle(frame, predicted_center, radius, (0, 0, 255), 5)\\n    \\n\\n    # Get original video dimensions\\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\\n    \\n    # Set a desired maximum window width or height, while maintaining the aspect ratio\\n    desired_width = 1000  # Change this as needed\\n    \\n    # Calculate the appropriate height based on the aspect ratio\\n    aspect_ratio = frame_height / frame_width\\n    new_height = int(desired_width * aspect_ratio)\\n    \\n    # Set up the window for display and resize it, keeping the aspect ratio\\n    cv2.namedWindow(\\'Frame\\', cv2.WINDOW_NORMAL)  # Create a window\\n    cv2.resizeWindow(\\'Frame\\', desired_width, new_height)  # Resize keeping the aspect ratio\\n\\n    # Show the frame\\n    cv2.imshow(\\'Frame\\', frame)\\n    cv2.waitKey(150)\\n    \\ncap.release()\\ncv2.destroyAllWindows()'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def filter_red_color(frame):\n",
    "    # Convert the frame to HSV\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define the red color range\n",
    "    lower_red1 = np.array([0, 100, 50])\n",
    "    upper_red1 = np.array([7, 255, 255])\n",
    "    lower_red2 = np.array([173, 100, 50])\n",
    "    upper_red2 = np.array([180, 255, 255])\n",
    "\n",
    "    # Create masks for both red ranges\n",
    "    mask1 = cv2.inRange(hsv, lower_red1, upper_red1)\n",
    "    mask2 = cv2.inRange(hsv, lower_red2, upper_red2)\n",
    "\n",
    "    # Combine the two masks\n",
    "    mask = mask1 + mask2\n",
    "\n",
    "    # Apply the mask to get only red parts of the image\n",
    "    result = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "\n",
    "    return result\n",
    "\n",
    "def initialize_kalman():\n",
    "    kalman = {\n",
    "        \"x\": np.array([0,\n",
    "              0,\n",
    "              0,\n",
    "              0,\n",
    "              0,\n",
    "              0]),  # State vector\n",
    "        \"P\": 1000 * np.eye(6),  # Initial uncertainty, a random high number\n",
    "        \"F\": np.array([[1, 1, 0.5, 0, 0, 0],\n",
    "                       [0, 1, 1, 0, 0, 0],\n",
    "                       [0, 0, 1, 0, 0, 0],\n",
    "                       [0, 0, 0, 1, 1, 0.5],\n",
    "                       [0, 0, 0, 0, 1, 1],\n",
    "                       [0, 0, 0, 0, 0, 1]]),  # Transition matrix\n",
    "        \"u\": np.zeros(6),  # External motion\n",
    "        \"H\": np.array([[1, 0, 0, 0, 0, 0],  # Observe x position\n",
    "                       [0, 0, 0, 1, 0, 0]]),  # Observe y position\n",
    "        \"R\": np.eye(2),  # Measurement uncertainty\n",
    "        \"I\": np.eye(6)  # Identity matrix\n",
    "    }\n",
    "    return kalman\n",
    "\n",
    "# Function to update the Kalman filter\n",
    "def update(kalman, Z):\n",
    "    x, P, H, R, I = kalman[\"x\"], kalman[\"P\"], kalman[\"H\"], kalman[\"R\"], kalman[\"I\"]\n",
    "    \n",
    "    # Measurement residual y\n",
    "    y = Z - np.dot(H, x)\n",
    "    \n",
    "    # Residual covariance S\n",
    "    S = np.dot(H, np.dot(P, H.T)) + R\n",
    "    \n",
    "    # Kalman gain K\n",
    "    K = np.dot(P, np.dot(H.T, np.linalg.inv(S)))\n",
    "    \n",
    "    # Update state estimate x\n",
    "    x = x + np.dot(K, y)\n",
    "    \n",
    "    # Update uncertainty P\n",
    "    P = np.dot(I - np.dot(K, H), P)\n",
    "    \n",
    "    kalman[\"x\"], kalman[\"P\"] = x, P\n",
    "    return kalman\n",
    "\n",
    "# Function to predict the next state\n",
    "def predict(kalman):\n",
    "    x, P, F, u = kalman[\"x\"], kalman[\"P\"], kalman[\"F\"], kalman[\"u\"]\n",
    "    \n",
    "    # Predict state x\n",
    "    x = np.dot(F, x) + u\n",
    "    \n",
    "    # Predict uncertainty P\n",
    "    P = np.dot(F, np.dot(P, F.T))\n",
    "    \n",
    "    kalman[\"x\"], kalman[\"P\"] = x, P\n",
    "    return kalman\n",
    "\n",
    "import cv2\n",
    "\n",
    "def draw_multiple_bounding_boxes(image, boxes, color=(0, 255, 0), thickness=2):\n",
    "    \"\"\"\n",
    "    Draws multiple bounding boxes on an image.\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy array): The image on which the bounding boxes will be drawn.\n",
    "        boxes (list): A list of dictionaries. Each dictionary should contain:\n",
    "                      - 'center': A tuple (cx, cy) representing the center of the bounding box.\n",
    "                      - 'width': The width of the bounding box.\n",
    "                      - 'height': The height of the bounding box.\n",
    "                      - 'label' (optional): A string to display above the bounding box.\n",
    "        color (tuple): The color of the bounding boxes in BGR format (default is green).\n",
    "        thickness (int): The thickness of the bounding box lines (default is 2).\n",
    "\n",
    "    Returns:\n",
    "        numpy array: The image with the bounding boxes drawn.\n",
    "    \"\"\"\n",
    "    for box in boxes:\n",
    "        # Extract parameters\n",
    "        center = box['center']\n",
    "        width = box['width']\n",
    "        height = box['height']\n",
    "        #label = box.get('label', None)  # Optional label\n",
    "        \n",
    "        # Calculate top-left and bottom-right coordinates\n",
    "        cx, cy = center\n",
    "        x1 = int(cx - width / 2)\n",
    "        y1 = int(cy - height / 2)\n",
    "        x2 = int(cx + width / 2)\n",
    "        y2 = int(cy + height / 2)\n",
    "        \n",
    "        # Draw the rectangle\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness)\n",
    "        \n",
    "        # Draw the label if provided\n",
    "        if label:\n",
    "            cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, thickness)\n",
    "    \n",
    "    return image\n",
    "\n",
    "'''\n",
    "# Load the video\n",
    "cap = cv2.VideoCapture('rolling_ball.mp4')\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open video\")\n",
    "    exit()\n",
    "\n",
    "# Sharpening kernel\n",
    "# to emphasize the central pixels\n",
    "sharpen_kernel = np.array([[-1, -1, -1],\n",
    "                           [-1,  9, -1],\n",
    "                           [-1, -1, -1]])\n",
    "\n",
    "# Initialize radius with a default value\n",
    "radius = 0\n",
    "\n",
    "# Looping through all the frames\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    ### Detect the ball ###\n",
    "    # Filter by red color first\n",
    "    red_frame = filter_red_color(frame)\n",
    "\n",
    "    # Convert the red frame to grayscale for edge detection\n",
    "    gray = cv2.cvtColor(red_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply a Gaussian blur\n",
    "    gray_blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Detect edges\n",
    "    edges = cv2.Canny(gray_blurred, 30, 50)\n",
    "\n",
    "    # Apply HoughCircles to detect circles\n",
    "    pieces = cv2.HoughCircles(edges, cv2.HOUGH_GRADIENT, 1, 80, param1=100, param2=20, minRadius=40, maxRadius=125)\n",
    "\n",
    "    # We now display the ball if it was found\n",
    "    if pieces is not None:\n",
    "        # Convert the (x, y, radius) values of the circles to integers\n",
    "        pieces = np.uint16(np.around(pieces))\n",
    "    \n",
    "        # Draw the circles on the original image\n",
    "        circle = pieces[0, 0]\n",
    "        center = (circle[0], circle[1])\n",
    "        radius = circle[2]\n",
    "\n",
    "        cv2.circle(frame, center, radius, (0, 255, 0), 5)  # Green circle with thickness 2\n",
    "    \n",
    "        ### If the ball is found, update the Kalman filter ###\n",
    "        Z = np.array(center)  # Z holds the \"measured position\" of the ball\n",
    "        x, P = update(x, P, Z, H, R)\n",
    "    \n",
    "    ### Predict the next state\n",
    "    x, P = predict(x, P, F, u)\n",
    "    predicted_center = (int(x[0]), int(x[3]))\n",
    "    ### Draw the predicted state on the image frame ###\n",
    "    cv2.circle(frame, predicted_center, radius, (0, 0, 255), 5)\n",
    "    \n",
    "\n",
    "    # Get original video dimensions\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Set a desired maximum window width or height, while maintaining the aspect ratio\n",
    "    desired_width = 1000  # Change this as needed\n",
    "    \n",
    "    # Calculate the appropriate height based on the aspect ratio\n",
    "    aspect_ratio = frame_height / frame_width\n",
    "    new_height = int(desired_width * aspect_ratio)\n",
    "    \n",
    "    # Set up the window for display and resize it, keeping the aspect ratio\n",
    "    cv2.namedWindow('Frame', cv2.WINDOW_NORMAL)  # Create a window\n",
    "    cv2.resizeWindow('Frame', desired_width, new_height)  # Resize keeping the aspect ratio\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.waitKey(150)\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ee3eb58-45cf-466f-a080-b620948fc095",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes=[]\n",
    "for label,x1, y1, x2, y2, x_center, y_center,occlusion_rate in occlusion_rates:\n",
    "    if occlusion_rate < 2: # so 0 or 1\n",
    "        #kalman filter applied, we have to initialize the matrices (x,P,u,F,P...) for each bounding box\n",
    "        # and the state (position, velocity, etc.) and uncertainty (P matrix) for each object need to be tracked independently.\n",
    "        width = x2-x1\n",
    "        height = y2-y1\n",
    "        kalman = initialize_kalman()\n",
    "        Z = [x_center,y_center]   # it has to be 1x2 vector\n",
    "        kalman = update(kalman,Z)\n",
    "        kalman = predict(kalman)\n",
    "        new_x = kalman['x']\n",
    "        new_center = (round(new_x[0]),round(new_x[3]))\n",
    "        # now we have to expand the dictionary putting in the new coordinate of predicted the center\n",
    "        boxes.append({new_center, width, height})\n",
    "    else:\n",
    "       continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220be8e7-28fc-44a5-b8b7-8af91932e2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
