{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pipeline for object detection and tracking in 3D:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Required packages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt \n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from itertools import zip_longest\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.general import (Profile, cv2, non_max_suppression, scale_boxes, xyxy2xywh)\n",
    "from utils.augmentations import letterbox\n",
    "from utils.torch_utils import select_device, smart_inference_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Definition of Objects and Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2D Object detection : YOLOV5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE PARAMETERS:\n",
    "\n",
    "weights=\"weights/X-512.pt\"\n",
    "source_1=\"data/view1\"\n",
    "source_2=\"data/view2\"\n",
    "device=\"cpu\"\n",
    "project=\"runs\"\n",
    "name=\"exp\"\n",
    "\n",
    "image_size=(512, 512)\n",
    "conf_thres=0.25\n",
    "max_det=20\n",
    "line_thickness=2\n",
    "iou_thres=0.45\n",
    "\n",
    "save_txt=True\n",
    "save_csv=False\n",
    "nosave=False\n",
    "hide_labels=False \n",
    "hide_conf=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetector():\n",
    "    \n",
    "    def __init__(self, device, weights, source_1, source_2, image_size):\n",
    "        self.device = select_device(device)\n",
    "        self.model = DetectMultiBackend(weights, self.device)\n",
    "        self.stride, self.names, self.pt = self.model.stride, self.model.names, self.model.pt\n",
    "        self.imgsz = image_size\n",
    "        self.model.warmup(imgsz=(1 , 3, *self.imgsz))\n",
    "        self.dt = Profile(device=self.device)\n",
    "        self.source_1 = Path(source_1)\n",
    "        self.source_2 = Path(source_2)\n",
    "        self.files_1 = [f for f in self.source_1.glob('*') if f.suffix.lower() in ['.jpg', '.jpeg', '.png']]\n",
    "        self.files_2 = [f for f in self.source_2.glob('*') if f.suffix.lower() in ['.jpg', '.jpeg', '.png']]\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for file_1, file_2 in zip_longest(self.files_1, self.files_2, fillvalue=None):\n",
    "            img_1 = cv2.imread(str(file_1))\n",
    "            img_2 = cv2.imread(str(file_2))\n",
    "            yield (img_1, img_2)\n",
    "    \n",
    "    @smart_inference_mode()\n",
    "    def detect_object_2D(self, im):\n",
    "        with self.dt:\n",
    "            image_original = im\n",
    "            im = letterbox(im, self.imgsz, stride=self.stride, auto=True)[0]\n",
    "            im = im.transpose((2, 0, 1))[::-1]\n",
    "            im = np.ascontiguousarray(im)\n",
    "            im = torch.from_numpy(im).to(self.device)\n",
    "            im = im.float().unsqueeze(0)\n",
    "            im /= 255\n",
    "                        \n",
    "            pred = self.model(im)\n",
    "            pred = non_max_suppression(pred)\n",
    "            \n",
    "            for det in pred:\n",
    "                gn = torch.tensor(image_original.shape)[[1, 0, 1, 0]] #Normalization\n",
    "                det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], image_original.shape).round() # Rescale to original size\n",
    "                for *xyxy, conf, cls in reversed(det):\n",
    "                    coords = ((xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist())  # normalized xywh\n",
    "                    line = (cls, *coords)\n",
    "                    print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-383-g1435a8ee Python-3.10.5 torch-2.5.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 484 layers, 88417530 parameters, 0 gradients\n"
     ]
    }
   ],
   "source": [
    "detector = ObjectDetector(device, weights, source_1, source_2, image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Z ESTIMATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_to_object(left_bbox, right_bbox, im_left, im_right, focal_length = 707.0493, baseline = 0.06):\n",
    "    \"\"\"\n",
    "    Calculate the distance to an object using bounding box coordinates from left and right images,\n",
    "    along with the camera calibration parameters (focal length and baseline).\n",
    "    \n",
    "    Parameters:\n",
    "        left_bbox (tuple): Bounding box coordinates in the left image (left, top, right, bottom)\n",
    "        right_bbox (tuple): Bounding box coordinates in the right image (left, top, right, bottom)\n",
    "        focal_length (float): Focal length of the camera (in pixels)\n",
    "        baseline (float): Baseline distance between cameras (in meters)\n",
    "        img_left (ndarray): Left image (grayscale)\n",
    "        img_right (ndarray): Right image (grayscale)\n",
    "        \n",
    "    Returns:\n",
    "        float: Distance to the object (in meters)\n",
    "        float: Disparity between the left and right images (in pixels)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the center of the bounding box in the left and right images\n",
    "    x_left = (left_bbox[0] + left_bbox[2]) / 2  # Average x of the left box\n",
    "    y_left = (left_bbox[1] + left_bbox[3]) / 2  # Average y of the left box\n",
    "    \n",
    "    x_right = (right_bbox[0] + right_bbox[2]) / 2  # Average x of the right box\n",
    "    y_right = (right_bbox[1] + right_bbox[3]) / 2  # Average y of the right box\n",
    "    \n",
    "    # Calculate disparity (horizontal pixel difference between the left and right image)\n",
    "    disparity = abs(x_left - x_right)\n",
    "    \n",
    "    if disparity == 0:\n",
    "        return float('inf'), disparity  # If disparity is zero, the object is too far away or at the same location\n",
    "    \n",
    "    # Calculate the distance to the object using the formula\n",
    "    distance = (focal_length * baseline) / disparity\n",
    "    \n",
    "    return distance, disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 0\n",
      "(tensor(3.), 0.10171568393707275, 0.6202702522277832, 0.2034313678741455, 0.7594594359397888)\n",
      "\n",
      "\n",
      "(tensor(1.), 0.4473039209842682, 0.4486486613750458, 0.025326797738671303, 0.13513512909412384)\n",
      "(tensor(1.), 0.38153594732284546, 0.47837838530540466, 0.02124183066189289, 0.18378378450870514)\n",
      "(tensor(1.), 0.16421568393707275, 0.43378376960754395, 0.013071895577013493, 0.062162160873413086)\n",
      "(tensor(1.), 0.42197713255882263, 0.44594594836235046, 0.01715686358511448, 0.1459459513425827)\n",
      "(tensor(1.), 0.4554738700389862, 0.4472973048686981, 0.018790850415825844, 0.13783784210681915)\n",
      "(tensor(1.), 0.31086599826812744, 0.7324324250221252, 0.09068627655506134, 0.47567567229270935)\n",
      "(tensor(3.), 0.07598039507865906, 0.6243243217468262, 0.15196079015731812, 0.7513513565063477)\n",
      "\n",
      "\n",
      "Image 1\n",
      "(tensor(3.), 0.1021241843700409, 0.6202702522277832, 0.2042483687400818, 0.7594594359397888)\n",
      "\n",
      "\n",
      "(tensor(1.), 0.42687907814979553, 0.44999998807907104, 0.023692810907959938, 0.1432432383298874)\n",
      "(tensor(1.), 0.44281044602394104, 0.44999998807907104, 0.019607843831181526, 0.13243243098258972)\n",
      "(tensor(1.), 0.16584967076778412, 0.4324324429035187, 0.011437908746302128, 0.05945945903658867)\n",
      "(tensor(1.), 0.4195261299610138, 0.44594594836235046, 0.018790850415825844, 0.1459459513425827)\n",
      "(tensor(1.), 0.45506536960601807, 0.4486486613750458, 0.016339870169758797, 0.1297297328710556)\n",
      "(tensor(3.), 0.07352941483259201, 0.6243243217468262, 0.14705882966518402, 0.7513513565063477)\n",
      "\n",
      "\n",
      "Image 2\n",
      "(tensor(1.), 0.46364378929138184, 0.45270270109176636, 0.02042483724653721, 0.1270270198583603)\n",
      "(tensor(1.), 0.44689542055130005, 0.541891872882843, 0.03594771400094032, 0.2243243306875229)\n",
      "(tensor(1.), 0.433415025472641, 0.6783784031867981, 0.0416666679084301, 0.44324323534965515)\n",
      "(tensor(1.), 0.37826797366142273, 0.6527026891708374, 0.04901960864663124, 0.49459460377693176)\n",
      "(tensor(3.), 0.1021241843700409, 0.6202702522277832, 0.2042483687400818, 0.7594594359397888)\n",
      "\n",
      "\n",
      "(tensor(1.), 0.41830065846443176, 0.4567567706108093, 0.022875817492604256, 0.15675675868988037)\n",
      "(tensor(1.), 0.40727123618125916, 0.45945945382118225, 0.02042483724653721, 0.1621621549129486)\n",
      "(tensor(1.), 0.44607841968536377, 0.4513513445854187, 0.02450980432331562, 0.13513512909412384)\n",
      "(tensor(1.), 0.4248366057872772, 0.4472973048686981, 0.016339870169758797, 0.14864864945411682)\n",
      "(tensor(1.), 0.16789215803146362, 0.43378376960754395, 0.01225490216165781, 0.062162160873413086)\n",
      "(tensor(1.), 0.3880718946456909, 0.6648648381233215, 0.04084967449307442, 0.44324323534965515)\n",
      "(tensor(3.), 0.07312091439962387, 0.6243243217468262, 0.14624182879924774, 0.7513513565063477)\n",
      "\n",
      "\n",
      "Image 3\n",
      "(tensor(1.), 0.39052286744117737, 0.6864864826202393, 0.053921569138765335, 0.5729729533195496)\n",
      "(tensor(3.), 0.10171568393707275, 0.6202702522277832, 0.2034313678741455, 0.7594594359397888)\n",
      "\n",
      "\n",
      "(tensor(1.), 0.41707515716552734, 0.4959459602832794, 0.03186274692416191, 0.21351350843906403)\n",
      "(tensor(1.), 0.44607841968536377, 0.454054057598114, 0.022875817492604256, 0.14054054021835327)\n",
      "(tensor(1.), 0.20302288234233856, 0.4324324429035187, 0.01225490216165781, 0.07567567378282547)\n",
      "(tensor(1.), 0.16993464529514313, 0.4351351261138916, 0.013071895577013493, 0.05945945903658867)\n",
      "(tensor(1.), 0.4236111044883728, 0.4486486613750458, 0.01715686358511448, 0.15135134756565094)\n",
      "(tensor(1.), 0.45383986830711365, 0.4513513445854187, 0.01715686358511448, 0.13513512909412384)\n",
      "(tensor(1.), 0.3991013169288635, 0.6729729771614075, 0.04983660206198692, 0.4702702760696411)\n",
      "(tensor(1.), 0.33905228972435, 0.7324324250221252, 0.050653595477342606, 0.4972972869873047)\n",
      "(tensor(3.), 0.07475490123033524, 0.6243243217468262, 0.14950980246067047, 0.7513513565063477)\n",
      "\n",
      "\n",
      "Image 4\n",
      "(tensor(1.), 0.4195261299610138, 0.44594594836235046, 0.01715686358511448, 0.13513512909412384)\n",
      "(tensor(1.), 0.45588234066963196, 0.6770270466804504, 0.07026144117116928, 0.47837838530540466)\n",
      "(tensor(1.), 0.39746731519699097, 0.6959459185600281, 0.05637254938483238, 0.5756756663322449)\n",
      "(tensor(3.), 0.10171568393707275, 0.6202702522277832, 0.2034313678741455, 0.7594594359397888)\n",
      "\n",
      "\n",
      "(tensor(1.), 0.41707515716552734, 0.454054057598114, 0.018790850415825844, 0.1459459513425827)\n",
      "(tensor(1.), 0.3986928164958954, 0.5405405163764954, 0.03758170083165169, 0.2864864766597748)\n",
      "(tensor(1.), 0.445669949054718, 0.45270270109176636, 0.023692810907959938, 0.13243243098258972)\n",
      "(tensor(1.), 0.17197711765766144, 0.43648648262023926, 0.01225490216165781, 0.062162160873413086)\n",
      "(tensor(1.), 0.4534313678741455, 0.44999998807907104, 0.016339870169758797, 0.13243243098258972)\n",
      "(tensor(1.), 0.34477123618125916, 0.7202702760696411, 0.047385621815919876, 0.5162162184715271)\n",
      "(tensor(1.), 0.41217321157455444, 0.6810810565948486, 0.0694444477558136, 0.4702702760696411)\n",
      "(tensor(3.), 0.07230392098426819, 0.6243243217468262, 0.14460784196853638, 0.7513513565063477)\n",
      "\n",
      "\n",
      "Image 5\n",
      "(tensor(1.), 0.46200981736183167, 0.6864864826202393, 0.07598039507865906, 0.5189189314842224)\n",
      "(tensor(1.), 0.4027777910232544, 0.6783784031867981, 0.06535948067903519, 0.5513513684272766)\n",
      "(tensor(3.), 0.10253267735242844, 0.6202702522277832, 0.20506535470485687, 0.7594594359397888)\n",
      "\n",
      "\n",
      "(tensor(1.), 0.17483660578727722, 0.49864864349365234, 0.014705882407724857, 0.13243243098258972)\n",
      "(tensor(1.), 0.4346405267715454, 0.545945942401886, 0.03594771400094032, 0.2108108103275299)\n",
      "(tensor(1.), 0.3950163424015045, 0.47567567229270935, 0.01552287582308054, 0.18378378450870514)\n",
      "(tensor(1.), 0.3999182879924774, 0.5283783674240112, 0.023692810907959938, 0.2891891896724701)\n",
      "(tensor(1.), 0.4493463933467865, 0.45540541410446167, 0.019607843831181526, 0.13783784210681915)\n",
      "(tensor(1.), 0.1736111044883728, 0.4378378391265869, 0.01225490216165781, 0.0648648664355278)\n",
      "(tensor(1.), 0.4174836575984955, 0.6986486315727234, 0.07026144117116928, 0.49459460377693176)\n",
      "(tensor(1.), 0.35580065846443176, 0.704054057598114, 0.0645424872636795, 0.5540540814399719)\n",
      "(tensor(3.), 0.07434640824794769, 0.6229729652404785, 0.14869281649589539, 0.754054069519043)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, images in enumerate(detector):\n",
    "    print(f'Image {i}')\n",
    "    detector.detect_object_2D(images[0])\n",
    "    print('\\n')\n",
    "    detector.detect_object_2D(images[1])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # 1. New frame (2)\n",
    "    frame = get_next_frame()\n",
    "    \n",
    "    # 2. Yolo detection in 2 stereo images\n",
    "    detections_2d = yolo_model.detect(frame)\n",
    "    \n",
    "    # 3.3D reconstruction (center x,y in 2 stereo images)\n",
    "    detections_3d = []\n",
    "    for bbox in detections_2d:\n",
    "\n",
    "        Z = disparity_to_depth(disparity_map[bbox_center_y1, bbox_center_x1, bbox_center_y2, bbox_center_x2 ])\n",
    "\n",
    "        X = (bbox_center_x - cx) * Z / fx\n",
    "        Y = (bbox_center_y - cy) * Z / fy\n",
    "        detections_3d.append(point_3d)\n",
    "    \n",
    "    # 4 detect oclusion (if oclusion kalman, if not linear interpolation of line tracker.)\n",
    "    \n",
    "        # 4A. UPDATE tracker\n",
    "        active_tracks = KALMAN.tracker.update(detections_3d)\n",
    "        \n",
    "        # 5A. visualize results:\n",
    "        for track_id, track in active_tracks.items():\n",
    "            position = track['kalman'].state[:3]  # Posición filtrada\n",
    "            velocity = track['kalman'].state[3:]  # Velocidad estimada\n",
    "            draw_track(frame, position, track_id)\n",
    "            \n",
    "        #4A linear interpolation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
