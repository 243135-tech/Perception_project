{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pipeline for object detection and tracking in 3D:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Required packages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "from sort import Sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Definition of Objects and Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2D Object detection : YOLOV5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x640 6 persons, 1 bicycle, 2 cars, 202.9ms\n",
      "Speed: 2.3ms preprocess, 202.9ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 bicycles, 2 cars, 174.8ms\n",
      "Speed: 0.8ms preprocess, 174.8ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 4 persons, 2 bicycles, 3 cars, 187.8ms\n",
      "Speed: 0.8ms preprocess, 187.8ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 persons, 2 bicycles, 2 cars, 243.5ms\n",
      "Speed: 0.9ms preprocess, 243.5ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 persons, 2 bicycles, 3 cars, 209.5ms\n",
      "Speed: 0.7ms preprocess, 209.5ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 persons, 2 bicycles, 2 cars, 231.1ms\n",
      "Speed: 0.9ms preprocess, 231.1ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 4 persons, 2 bicycles, 2 cars, 282.6ms\n",
      "Speed: 0.8ms preprocess, 282.6ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 bicycles, 2 cars, 336.5ms\n",
      "Speed: 0.7ms preprocess, 336.5ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 1 bicycle, 2 cars, 204.5ms\n",
      "Speed: 1.0ms preprocess, 204.5ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 1 bicycle, 2 cars, 211.3ms\n",
      "Speed: 0.8ms preprocess, 211.3ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 persons, 1 bicycle, 2 cars, 262.5ms\n",
      "Speed: 1.7ms preprocess, 262.5ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 cars, 245.6ms\n",
      "Speed: 1.0ms preprocess, 245.6ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 3 cars, 243.2ms\n",
      "Speed: 0.9ms preprocess, 243.2ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 3 cars, 207.7ms\n",
      "Speed: 0.8ms preprocess, 207.7ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 persons, 3 cars, 267.8ms\n",
      "Speed: 0.9ms preprocess, 267.8ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 1 bicycle, 3 cars, 181.5ms\n",
      "Speed: 0.9ms preprocess, 181.5ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 3 cars, 192.7ms\n",
      "Speed: 0.8ms preprocess, 192.7ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 190.4ms\n",
      "Speed: 0.7ms preprocess, 190.4ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 3 cars, 197.2ms\n",
      "Speed: 0.8ms preprocess, 197.2ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 3 cars, 231.5ms\n",
      "Speed: 1.4ms preprocess, 231.5ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 1 car, 225.6ms\n",
      "Speed: 1.0ms preprocess, 225.6ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 1 bicycle, 2 cars, 269.1ms\n",
      "Speed: 3.3ms preprocess, 269.1ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 2 cars, 271.3ms\n",
      "Speed: 1.1ms preprocess, 271.3ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 1 bicycle, 2 cars, 258.1ms\n",
      "Speed: 1.2ms preprocess, 258.1ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 1 bicycle, 2 cars, 191.7ms\n",
      "Speed: 0.8ms preprocess, 191.7ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 9 persons, 2 cars, 214.4ms\n",
      "Speed: 0.8ms preprocess, 214.4ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 9 persons, 2 cars, 287.7ms\n",
      "Speed: 0.9ms preprocess, 287.7ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 9 persons, 2 cars, 194.4ms\n",
      "Speed: 0.8ms preprocess, 194.4ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 1 car, 201.8ms\n",
      "Speed: 0.8ms preprocess, 201.8ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 2 cars, 193.1ms\n",
      "Speed: 0.9ms preprocess, 193.1ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 211.5ms\n",
      "Speed: 1.2ms preprocess, 211.5ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 1 car, 183.1ms\n",
      "Speed: 0.7ms preprocess, 183.1ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 1 car, 210.0ms\n",
      "Speed: 0.8ms preprocess, 210.0ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 187.7ms\n",
      "Speed: 1.1ms preprocess, 187.7ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 1 car, 215.2ms\n",
      "Speed: 0.9ms preprocess, 215.2ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 1 car, 181.6ms\n",
      "Speed: 0.8ms preprocess, 181.6ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 1 car, 230.3ms\n",
      "Speed: 0.7ms preprocess, 230.3ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 184.2ms\n",
      "Speed: 0.8ms preprocess, 184.2ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 3 cars, 243.2ms\n",
      "Speed: 0.8ms preprocess, 243.2ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 3 cars, 228.8ms\n",
      "Speed: 4.3ms preprocess, 228.8ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 304.6ms\n",
      "Speed: 34.2ms preprocess, 304.6ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 261.4ms\n",
      "Speed: 0.9ms preprocess, 261.4ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 cars, 401.3ms\n",
      "Speed: 1.8ms preprocess, 401.3ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 3 cars, 333.1ms\n",
      "Speed: 0.9ms preprocess, 333.1ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 3 cars, 224.2ms\n",
      "Speed: 1.8ms preprocess, 224.2ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 cars, 230.0ms\n",
      "Speed: 2.4ms preprocess, 230.0ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 cars, 213.9ms\n",
      "Speed: 1.1ms preprocess, 213.9ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 cars, 327.5ms\n",
      "Speed: 0.9ms preprocess, 327.5ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 cars, 259.2ms\n",
      "Speed: 0.8ms preprocess, 259.2ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 cars, 389.4ms\n",
      "Speed: 8.1ms preprocess, 389.4ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 cars, 357.7ms\n",
      "Speed: 0.8ms preprocess, 357.7ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 persons, 2 cars, 327.6ms\n",
      "Speed: 1.7ms preprocess, 327.6ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 persons, 3 cars, 395.3ms\n",
      "Speed: 0.8ms preprocess, 395.3ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 cars, 256.8ms\n",
      "Speed: 0.7ms preprocess, 256.8ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 cars, 225.9ms\n",
      "Speed: 1.1ms preprocess, 225.9ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 184.0ms\n",
      "Speed: 1.0ms preprocess, 184.0ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 2 cars, 225.9ms\n",
      "Speed: 0.8ms preprocess, 225.9ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 214.9ms\n",
      "Speed: 1.1ms preprocess, 214.9ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 249.1ms\n",
      "Speed: 1.5ms preprocess, 249.1ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 275.0ms\n",
      "Speed: 1.0ms preprocess, 275.0ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 263.1ms\n",
      "Speed: 0.9ms preprocess, 263.1ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 cars, 331.4ms\n",
      "Speed: 0.9ms preprocess, 331.4ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 3 cars, 508.7ms\n",
      "Speed: 0.7ms preprocess, 508.7ms inference, 1.4ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 3 cars, 326.0ms\n",
      "Speed: 1.6ms preprocess, 326.0ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 3 cars, 322.2ms\n",
      "Speed: 1.2ms preprocess, 322.2ms inference, 1.4ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 332.2ms\n",
      "Speed: 2.0ms preprocess, 332.2ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 218.9ms\n",
      "Speed: 1.2ms preprocess, 218.9ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 3 cars, 248.1ms\n",
      "Speed: 0.9ms preprocess, 248.1ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 216.3ms\n",
      "Speed: 5.9ms preprocess, 216.3ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 2 cars, 192.5ms\n",
      "Speed: 0.8ms preprocess, 192.5ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 3 cars, 203.1ms\n",
      "Speed: 0.8ms preprocess, 203.1ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 2 cars, 233.4ms\n",
      "Speed: 0.9ms preprocess, 233.4ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 334.0ms\n",
      "Speed: 1.1ms preprocess, 334.0ms inference, 1.9ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 309.7ms\n",
      "Speed: 0.8ms preprocess, 309.7ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 2 cars, 399.9ms\n",
      "Speed: 1.2ms preprocess, 399.9ms inference, 2.1ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 358.6ms\n",
      "Speed: 0.9ms preprocess, 358.6ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 297.0ms\n",
      "Speed: 0.8ms preprocess, 297.0ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 2 cars, 278.2ms\n",
      "Speed: 0.8ms preprocess, 278.2ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 2 cars, 259.4ms\n",
      "Speed: 0.8ms preprocess, 259.4ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 2 cars, 237.5ms\n",
      "Speed: 0.9ms preprocess, 237.5ms inference, 5.2ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 276.0ms\n",
      "Speed: 0.8ms preprocess, 276.0ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 2 cars, 216.6ms\n",
      "Speed: 2.1ms preprocess, 216.6ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 cars, 207.6ms\n",
      "Speed: 0.9ms preprocess, 207.6ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 233.3ms\n",
      "Speed: 0.8ms preprocess, 233.3ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 2 cars, 252.8ms\n",
      "Speed: 0.9ms preprocess, 252.8ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 207.8ms\n",
      "Speed: 1.2ms preprocess, 207.8ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 2 cars, 298.1ms\n",
      "Speed: 0.9ms preprocess, 298.1ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 226.9ms\n",
      "Speed: 0.8ms preprocess, 226.9ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 cars, 229.4ms\n",
      "Speed: 1.1ms preprocess, 229.4ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 cars, 325.3ms\n",
      "Speed: 5.0ms preprocess, 325.3ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 persons, 2 cars, 239.5ms\n",
      "Speed: 0.7ms preprocess, 239.5ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 343.6ms\n",
      "Speed: 1.0ms preprocess, 343.6ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 2 cars, 375.3ms\n",
      "Speed: 2.8ms preprocess, 375.3ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 299.8ms\n",
      "Speed: 2.1ms preprocess, 299.8ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 344.1ms\n",
      "Speed: 1.0ms preprocess, 344.1ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 2 cars, 213.5ms\n",
      "Speed: 0.9ms preprocess, 213.5ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 9 persons, 2 cars, 238.5ms\n",
      "Speed: 0.7ms preprocess, 238.5ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 326.7ms\n",
      "Speed: 0.9ms preprocess, 326.7ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 cars, 424.7ms\n",
      "Speed: 2.0ms preprocess, 424.7ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 persons, 2 cars, 380.6ms\n",
      "Speed: 0.9ms preprocess, 380.6ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 cars, 305.3ms\n",
      "Speed: 0.8ms preprocess, 305.3ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 cars, 239.4ms\n",
      "Speed: 4.8ms preprocess, 239.4ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 persons, 2 cars, 357.0ms\n",
      "Speed: 0.9ms preprocess, 357.0ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 persons, 2 cars, 260.1ms\n",
      "Speed: 1.3ms preprocess, 260.1ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 persons, 2 cars, 345.1ms\n",
      "Speed: 1.1ms preprocess, 345.1ms inference, 1.4ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 4 persons, 3 cars, 287.9ms\n",
      "Speed: 0.9ms preprocess, 287.9ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 3 cars, 373.9ms\n",
      "Speed: 1.2ms preprocess, 373.9ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 cars, 208.1ms\n",
      "Speed: 0.8ms preprocess, 208.1ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 cars, 310.1ms\n",
      "Speed: 1.1ms preprocess, 310.1ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 cars, 325.7ms\n",
      "Speed: 1.0ms preprocess, 325.7ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 364.2ms\n",
      "Speed: 0.9ms preprocess, 364.2ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 1 bicycle, 2 cars, 432.8ms\n",
      "Speed: 0.7ms preprocess, 432.8ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 2 cars, 272.0ms\n",
      "Speed: 31.9ms preprocess, 272.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 cars, 317.3ms\n",
      "Speed: 1.1ms preprocess, 317.3ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 9 persons, 2 cars, 336.1ms\n",
      "Speed: 1.7ms preprocess, 336.1ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 1 car, 337.2ms\n",
      "Speed: 2.4ms preprocess, 337.2ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 2 cars, 248.6ms\n",
      "Speed: 1.3ms preprocess, 248.6ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 1 car, 329.1ms\n",
      "Speed: 1.0ms preprocess, 329.1ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 9 persons, 1 car, 189.7ms\n",
      "Speed: 1.2ms preprocess, 189.7ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 1 car, 313.6ms\n",
      "Speed: 1.0ms preprocess, 313.6ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 9 persons, 326.3ms\n",
      "Speed: 0.8ms preprocess, 326.3ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 2 cars, 299.6ms\n",
      "Speed: 1.2ms preprocess, 299.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 1 car, 351.8ms\n",
      "Speed: 1.0ms preprocess, 351.8ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 10 persons, 1 bicycle, 1 car, 284.1ms\n",
      "Speed: 4.0ms preprocess, 284.1ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 10 persons, 1 bicycle, 2 cars, 220.3ms\n",
      "Speed: 1.2ms preprocess, 220.3ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 13 persons, 3 bicycles, 2 cars, 262.0ms\n",
      "Speed: 0.9ms preprocess, 262.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 11 persons, 3 bicycles, 1 car, 356.9ms\n",
      "Speed: 1.0ms preprocess, 356.9ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 12 persons, 2 bicycles, 1 car, 275.6ms\n",
      "Speed: 1.2ms preprocess, 275.6ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 11 persons, 2 bicycles, 2 cars, 284.2ms\n",
      "Speed: 1.5ms preprocess, 284.2ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 9 persons, 1 bicycle, 1 car, 307.9ms\n",
      "Speed: 1.2ms preprocess, 307.9ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 10 persons, 2 bicycles, 187.3ms\n",
      "Speed: 3.1ms preprocess, 187.3ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 10 persons, 3 bicycles, 1 car, 194.2ms\n",
      "Speed: 0.8ms preprocess, 194.2ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 1 bicycle, 1 car, 273.4ms\n",
      "Speed: 0.8ms preprocess, 273.4ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 bicycles, 1 car, 234.9ms\n",
      "Speed: 1.0ms preprocess, 234.9ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 1 bicycle, 1 car, 347.4ms\n",
      "Speed: 5.4ms preprocess, 347.4ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 1 bicycle, 2 cars, 317.1ms\n",
      "Speed: 1.2ms preprocess, 317.1ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 2 bicycles, 2 cars, 275.0ms\n",
      "Speed: 0.9ms preprocess, 275.0ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 bicycles, 2 cars, 273.8ms\n",
      "Speed: 5.6ms preprocess, 273.8ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 bicycles, 2 cars, 239.0ms\n",
      "Speed: 0.8ms preprocess, 239.0ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 bicycles, 2 cars, 277.3ms\n",
      "Speed: 1.1ms preprocess, 277.3ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 bicycles, 2 cars, 303.0ms\n",
      "Speed: 1.1ms preprocess, 303.0ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 bicycles, 2 cars, 231.6ms\n",
      "Speed: 1.7ms preprocess, 231.6ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 bicycles, 3 cars, 376.4ms\n",
      "Speed: 1.0ms preprocess, 376.4ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 persons, 2 bicycles, 2 cars, 252.7ms\n",
      "Speed: 0.9ms preprocess, 252.7ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 3 bicycles, 2 cars, 341.1ms\n",
      "Speed: 0.8ms preprocess, 341.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 persons, 2 bicycles, 2 cars, 273.2ms\n",
      "Speed: 1.0ms preprocess, 273.2ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 persons, 2 bicycles, 2 cars, 271.1ms\n",
      "Speed: 1.2ms preprocess, 271.1ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 4 persons, 3 bicycles, 2 cars, 422.4ms\n",
      "Speed: 8.0ms preprocess, 422.4ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 persons, 2 bicycles, 2 cars, 206.2ms\n",
      "Speed: 1.0ms preprocess, 206.2ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 persons, 3 bicycles, 2 cars, 300.4ms\n",
      "Speed: 0.8ms preprocess, 300.4ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 persons, 2 bicycles, 1 car, 342.1ms\n",
      "Speed: 0.8ms preprocess, 342.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 persons, 1 bicycle, 2 cars, 355.0ms\n",
      "Speed: 4.7ms preprocess, 355.0ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 1 bicycle, 2 cars, 323.7ms\n",
      "Speed: 0.8ms preprocess, 323.7ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 1 bicycle, 2 cars, 240.8ms\n",
      "Speed: 1.2ms preprocess, 240.8ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 1 bicycle, 2 cars, 189.5ms\n",
      "Speed: 0.8ms preprocess, 189.5ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 1 car, 224.7ms\n",
      "Speed: 0.7ms preprocess, 224.7ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 1 bicycle, 1 car, 322.3ms\n",
      "Speed: 1.8ms preprocess, 322.3ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 persons, 1 car, 303.3ms\n",
      "Speed: 4.7ms preprocess, 303.3ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 1 car, 289.3ms\n",
      "Speed: 0.9ms preprocess, 289.3ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 1 bicycle, 1 car, 298.6ms\n",
      "Speed: 0.8ms preprocess, 298.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 1 bicycle, 308.8ms\n",
      "Speed: 0.8ms preprocess, 308.8ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 bicycles, 299.2ms\n",
      "Speed: 1.1ms preprocess, 299.2ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 bicycles, 283.3ms\n",
      "Speed: 5.5ms preprocess, 283.3ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 bicycles, 297.2ms\n",
      "Speed: 0.9ms preprocess, 297.2ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 bicycles, 1 car, 308.8ms\n",
      "Speed: 0.8ms preprocess, 308.8ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 bicycles, 2 cars, 329.5ms\n",
      "Speed: 0.9ms preprocess, 329.5ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 bicycles, 1 car, 335.9ms\n",
      "Speed: 0.9ms preprocess, 335.9ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 3 bicycles, 1 car, 204.4ms\n",
      "Speed: 1.1ms preprocess, 204.4ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 3 bicycles, 281.9ms\n",
      "Speed: 0.8ms preprocess, 281.9ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 1 bicycle, 341.3ms\n",
      "Speed: 0.8ms preprocess, 341.3ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 2 bicycles, 324.3ms\n",
      "Speed: 1.3ms preprocess, 324.3ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 3 bicycles, 1 car, 291.3ms\n",
      "Speed: 1.3ms preprocess, 291.3ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 persons, 2 bicycles, 255.3ms\n",
      "Speed: 0.8ms preprocess, 255.3ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 persons, 3 bicycles, 2 cars, 337.2ms\n",
      "Speed: 0.7ms preprocess, 337.2ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 bicycles, 1 car, 407.5ms\n",
      "Speed: 0.8ms preprocess, 407.5ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 persons, 4 bicycles, 2 cars, 305.5ms\n",
      "Speed: 2.7ms preprocess, 305.5ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 3 bicycles, 2 cars, 261.8ms\n",
      "Speed: 1.0ms preprocess, 261.8ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 4 persons, 3 bicycles, 1 car, 304.3ms\n",
      "Speed: 1.1ms preprocess, 304.3ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 persons, 3 bicycles, 1 car, 310.3ms\n",
      "Speed: 0.9ms preprocess, 310.3ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 3 bicycles, 214.2ms\n",
      "Speed: 0.9ms preprocess, 214.2ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 persons, 2 bicycles, 1 car, 258.9ms\n",
      "Speed: 0.9ms preprocess, 258.9ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 persons, 3 bicycles, 1 car, 316.0ms\n",
      "Speed: 1.4ms preprocess, 316.0ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 4 persons, 1 bicycle, 2 cars, 302.2ms\n",
      "Speed: 0.9ms preprocess, 302.2ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 3 persons, 2 bicycles, 1 car, 381.2ms\n",
      "Speed: 1.1ms preprocess, 381.2ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 4 persons, 1 bicycle, 1 car, 261.7ms\n",
      "Speed: 0.9ms preprocess, 261.7ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 3 persons, 2 bicycles, 1 car, 311.4ms\n",
      "Speed: 1.0ms preprocess, 311.4ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 4 persons, 2 bicycles, 1 car, 364.9ms\n",
      "Speed: 1.1ms preprocess, 364.9ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 3 persons, 2 cars, 317.6ms\n",
      "Speed: 0.9ms preprocess, 317.6ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 3 persons, 2 cars, 382.1ms\n",
      "Speed: 5.1ms preprocess, 382.1ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 3 persons, 2 cars, 279.6ms\n",
      "Speed: 3.5ms preprocess, 279.6ms inference, 1.9ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 2 persons, 1 car, 250.0ms\n",
      "Speed: 0.7ms preprocess, 250.0ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 3 persons, 1 bicycle, 1 car, 315.7ms\n",
      "Speed: 1.0ms preprocess, 315.7ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 2 persons, 2 cars, 258.6ms\n",
      "Speed: 0.8ms preprocess, 258.6ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 3 persons, 2 cars, 352.6ms\n",
      "Speed: 0.9ms preprocess, 352.6ms inference, 2.3ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 3 persons, 2 cars, 225.6ms\n",
      "Speed: 1.1ms preprocess, 225.6ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 2 persons, 2 cars, 345.8ms\n",
      "Speed: 0.8ms preprocess, 345.8ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 2 persons, 4 cars, 224.5ms\n",
      "Speed: 1.1ms preprocess, 224.5ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 1 person, 3 cars, 326.9ms\n",
      "Speed: 0.9ms preprocess, 326.9ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 2 persons, 1 bicycle, 2 cars, 350.9ms\n",
      "Speed: 1.1ms preprocess, 350.9ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 2 persons, 3 cars, 278.9ms\n",
      "Speed: 1.0ms preprocess, 278.9ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 3 persons, 2 cars, 235.1ms\n",
      "Speed: 0.7ms preprocess, 235.1ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 2 persons, 2 cars, 266.3ms\n",
      "Speed: 1.9ms preprocess, 266.3ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 2 persons, 2 cars, 213.7ms\n",
      "Speed: 1.0ms preprocess, 213.7ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 3 persons, 1 bicycle, 2 cars, 224.2ms\n",
      "Speed: 0.8ms preprocess, 224.2ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 3 persons, 1 bicycle, 2 cars, 195.1ms\n",
      "Speed: 0.8ms preprocess, 195.1ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 3 persons, 1 bicycle, 2 cars, 244.9ms\n",
      "Speed: 1.0ms preprocess, 244.9ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 1 person, 3 cars, 243.1ms\n",
      "Speed: 0.8ms preprocess, 243.1ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 2 persons, 1 bicycle, 2 cars, 242.6ms\n",
      "Speed: 1.1ms preprocess, 242.6ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 3 persons, 1 bicycle, 2 cars, 347.8ms\n",
      "Speed: 3.0ms preprocess, 347.8ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 640)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the YOLO model\n",
    "model = YOLO(\"yolov8l.pt\")   # Moved to parameters \n",
    "\n",
    "# Initialize SORT tracker\n",
    "mot_tracker = Sort(max_age=3, min_hits=1, iou_threshold=0.5) # Not important for yolo\n",
    "def resize_image(input_image_path, width=217, height=225):\n",
    "    \"\"\"\n",
    "    Resizes an input image to the specified dimensions and saves the result.\n",
    "\n",
    "    Args:\n",
    "        input_image_path (str): Path to the input image.\n",
    "        output_image_path (str): Path to save the resized image.\n",
    "        width (int): Desired width of the resized image. Default is 217.\n",
    "        height (int): Desired height of the resized image. Default is 225.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load the input image\n",
    "    image = cv2.imread(input_image_path)\n",
    "\n",
    "    if image is None:\n",
    "        raise ValueError(\"Input image not found or unable to read.\")\n",
    "\n",
    "    # Resize the image to the specified dimensions\n",
    "    resized_image = cv2.resize(image, (width, height))\n",
    "\n",
    "    return resized_image\n",
    "\n",
    "overlap_image = resize_image(\"Overlap_image.png\",width=217, height=225)\n",
    "# Define input and obj folders\n",
    "set_img = 2\n",
    "if set_img == 1: # third sequence\n",
    "    output_folder = \"outputs1\"\n",
    "    data_folder_1 = \"data/view1\"  # Folder containing input frames\n",
    "    data_folder_2 = \"data/view2\"  # Folder containing input frames\n",
    "if set_img == 2: # second sequence\n",
    "    output_folder = \"outputs2\"\n",
    "    data_folder_1 = \"data/view3\"  # Folder containing input frames\n",
    "    data_folder_2 = \"data/view4\"  # Folder containing input frames\n",
    "else: # third sequence\n",
    "    output_folder = \"outputs3\"\n",
    "    data_folder_1 = \"data/view5\"  # Folder containing input frames\n",
    "    data_folder_2 = \"data/view6\"  # Folder containing input frames\n",
    "os.makedirs(output_folder, exist_ok=True)  # Create the obj folder if it doesn't exist\n",
    "\n",
    "def calculate_occlusion_area(box, overlay_rect):\n",
    "    x1 = max(box[0], overlay_rect[0])\n",
    "    y1 = max(box[1], overlay_rect[1])\n",
    "    x2 = min(box[2], overlay_rect[2])\n",
    "    y2 = min(box[3], overlay_rect[3])\n",
    "    \n",
    "    intersection_width = max(0, x2 - x1)\n",
    "    intersection_height = max(0, y2 - y1)\n",
    "    return intersection_width * intersection_height\n",
    "def initialize_kalman(x_center,y_center):\n",
    "    kalman = {\n",
    "        \"x\": np.array([0,\n",
    "              0,\n",
    "              0,\n",
    "              0]),  # State vector\n",
    "        \"P\": 1000 * np.eye(4),  # Initial uncertainty, a random high number\n",
    "        \"F\":  np.array([[1, 1, 0, 0],  # x_pos\n",
    "                        [0, 1, 0, 0],  # x_vel\n",
    "                        [0, 0, 1, 1], # y_pos\n",
    "                        [0, 0, 0, 1]]),  # y_vel # Transition matrix\n",
    "        \"u\": np.zeros(4),  # External motion\n",
    "        \"H\": np.array([[1, 0, 0, 0],  # Observe x position\n",
    "                       [0, 0, 1, 0]]),  # Observe y position\n",
    "        \"R\": 10 * np.eye(2),  # Measurement uncertainty\n",
    "        \"I\": np.eye(4)  # Identity matrix\n",
    "    }\n",
    "    return kalman\n",
    "def update(kalman, Z):\n",
    "    x, P, H, R, I = kalman[\"x\"], kalman[\"P\"], kalman[\"H\"], kalman[\"R\"], kalman[\"I\"]\n",
    "    \n",
    "    # Measurement residual y\n",
    "    y = Z - np.dot(H, x)\n",
    "    \n",
    "    # Residual covariance S\n",
    "    S = np.dot(H, np.dot(P, H.T)) + R\n",
    "    \n",
    "    # Kalman gain K\n",
    "    K = np.dot(P, np.dot(H.T, np.linalg.inv(S)))\n",
    "\n",
    "    # Update state estimate x\n",
    "    x = x + np.dot(K, y)\n",
    "    \n",
    "    # Update uncertainty P\n",
    "    P = np.dot(I - np.dot(K, H), P)\n",
    "    \n",
    "    kalman[\"x\"], kalman[\"P\"] = x, P\n",
    "    return kalman\n",
    "\n",
    "def predict(kalman):\n",
    "    x, P, F, u = kalman[\"x\"], kalman[\"P\"], kalman[\"F\"], kalman[\"u\"]\n",
    "    \n",
    "    Q = np.eye(4) * 0.1  # small noise\n",
    "\n",
    "    # Predict state x\n",
    "    x = np.dot(F, x) + u\n",
    "    \n",
    "    # Predict uncertainty P\n",
    "    P = np.dot(F, np.dot(P, F.T)) + Q\n",
    "    \n",
    "    kalman[\"x\"], kalman[\"P\"] = x, P\n",
    "    return kalman\n",
    "\n",
    "def new_kalman(track_id, tracked_predictions):\n",
    "    if track_id not in tracked_predictions:\n",
    "            kalman = initialize_kalman(x_center, y_center)\n",
    "            tracked_predictions[track_id] = {\"kalman\": kalman, \"width\": x2 - x1, \"height\": y2 - y1, \"occlusion_rate\":occlusion_rate,}\n",
    "    return tracked_predictions\n",
    "\n",
    "def create_outputs(outputs,occlusion_rate,frame_path,label,track_id,x1,y1,x2,y2,x_center,y_center):\n",
    "    if occlusion_rate == 100:\n",
    "            outputs.append((frame_path.name,label,track_id, x1, y1, x2, y2, x_center, y_center, 3)) # not visible at all\n",
    "    if occlusion_rate<100 and occlusion_rate >= 50:\n",
    "            outputs.append((frame_path.name,label,track_id, x1, y1, x2, y2, x_center, y_center, 2)) # partially visible\n",
    "    if occlusion_rate<50 and occlusion_rate > 0:\n",
    "            outputs.append((frame_path.name,label,track_id, x1, y1, x2, y2, x_center, y_center, 1)) # mostly visible\n",
    "    else:\n",
    "            outputs.append((frame_path.name,label,track_id, x1, y1, x2, y2, x_center, y_center, 0)) # totally visible\n",
    "    return outputs\n",
    "# Initialize a list to store occluded predictions\n",
    "tracked_predictions = {}\n",
    "outputs = []\n",
    "\n",
    "# Main loop for processing frames\n",
    "for frame_path in sorted(Path(data_folder_1).glob(\"*.png\")):\n",
    "    img = cv2.imread(str(frame_path))\n",
    "    ids = []\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not load image {frame_path.name}\")\n",
    "        continue\n",
    "\n",
    "    # Perform object detection\n",
    "    results = model(img, conf=0.5, classes=[0, 1, 2, 7])\n",
    "    detections = results[0].boxes\n",
    "\n",
    "    dets = []\n",
    "    labels = []\n",
    "\n",
    "    # Process each detection\n",
    "    for box in detections:\n",
    "        x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "        confidence = float(box.conf[0])\n",
    "        cls = int(box.cls[0])\n",
    "        label = model.names[cls]\n",
    "        labels.append(label)\n",
    "\n",
    "        if confidence < 0.5 or label not in [\"person\", \"car\", \"truck\",\"pedestrian\"]:\n",
    "            continue\n",
    "\n",
    "        dets.append([x1, y1, x2, y2, confidence])\n",
    "\n",
    "    dets = np.array(dets)\n",
    "\n",
    "    # Update SORT tracker\n",
    "    trackers = mot_tracker.update(dets)\n",
    "\n",
    "    # Convert both the frame and template to grayscale for template matching\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray_template = cv2.cvtColor(overlap_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    result = cv2.matchTemplate(img, overlap_image, cv2.TM_CCOEFF_NORMED)\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "    threshold = 0.4\n",
    "    overlay_rect = None\n",
    "    if max_val >= threshold:\n",
    "        top_left = (max_loc[0], max_loc[1])\n",
    "        h, w = gray_template.shape[:2]\n",
    "        bottom_right = (top_left[0] + w + 20, top_left[1] + h)\n",
    "        overlay_rect = (top_left[0], top_left[1], bottom_right[0], bottom_right[1])\n",
    "        cv2.rectangle(img, top_left, bottom_right, (0, 255, 0), 2)\n",
    "\n",
    "    # Process each tracked object\n",
    "    for i, d in enumerate(trackers): \n",
    "\n",
    "        ################ WE ARE DOING THIS FOR EACH TRACKED OBJECT\n",
    "\n",
    "        x1, y1, x2, y2, track_id = map(int, d)\n",
    "        ids.append(track_id)\n",
    "        x_center = (x1 + x2) / 2\n",
    "        y_center = (y1 + y2) / 2\n",
    "\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Draw bounding box\n",
    "        cv2.putText(img, f\": {label}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2) \n",
    "\n",
    "        # Calculate occlusion area\n",
    "        box_area = (x2 - x1) * (y2 - y1)\n",
    "        occlusion_area = calculate_occlusion_area((x1, y1, x2, y2), overlay_rect)\n",
    "        occlusion_rate = (occlusion_area / box_area) * 100 \n",
    "        outputs = create_outputs(outputs,occlusion_rate,frame_path,label,track_id, x1,y1,x2,y2,x_center,y_center)\n",
    "        #Check if this box is already being tracked for prediction\n",
    "        tracked_predictions = new_kalman(track_id, tracked_predictions)       \n",
    "        if occlusion_area == 0:\n",
    "            Z = np.array([x_center, y_center])\n",
    "            tracked_predictions[track_id]['kalman'] = update(tracked_predictions[track_id]['kalman'], Z) \n",
    "                # Update predictions for tracked boxes\n",
    "                # Also update the bounding box\n",
    "            tracked_predictions[track_id]['width'] = x2 - x1\n",
    "            tracked_predictions[track_id]['height'] = y2 - y1\n",
    "    \n",
    "    # Draw the predicted bounding box when the object is occluded, and also keep predicting\n",
    "    for track_id, prediction in tracked_predictions.items():\n",
    "        # Predict the next state\n",
    "        kalman = prediction['kalman']\n",
    "        kalman = predict(kalman) \n",
    "\n",
    "        # Update bounding box using the predicted position\n",
    "        x, y = kalman['x'][0] + 50, kalman['x'][2]\n",
    "        # increasing the velocity on the x-axis\n",
    "\n",
    "        width = prediction[\"width\"]\n",
    "        height = prediction[\"height\"]\n",
    "    \n",
    "        # Update bounding box dimensions based on the smoothed center\n",
    "        new_x1 = int(x - width / 2)\n",
    "        new_x2 = int(x + width / 2)\n",
    "        new_y1 = int(y - height / 2)\n",
    "        new_y2 = int(y + height / 2)\n",
    "\n",
    "        tracked_predictions[track_id]['kalman'] = kalman\n",
    "                \n",
    "        # Recalculate occlusion area   \n",
    "        box_area = width * height\n",
    "        occlusion_area = calculate_occlusion_area((new_x1, new_y1, new_x2, new_y2), overlay_rect)\n",
    "        occlusion_rate = (occlusion_area / box_area) * 100\n",
    "\n",
    "        tracked_predictions[track_id]['occlusion_rate'] = occlusion_rate\n",
    "    \n",
    "        if tracked_predictions[track_id]['occlusion_rate'] > 0 and box_area > 3000:\n",
    "            cv2.rectangle(img, (new_x1, new_y1), (new_x2, new_y2), (0, 0, 255), 2)\n",
    "            cv2.putText(img, f\"Pred: {track_id}\", (new_x1, new_y1 - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "    \n",
    "    # Save annotated frame\n",
    "    output_path = os.path.join(output_folder, frame_path.name)\n",
    "    cv2.imwrite(output_path, img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Kalman block\n",
    "def initialize_kalman(x_center,y_center):\n",
    "    kalman = {\n",
    "        \"x\": np.array([0,\n",
    "              0,\n",
    "              0,\n",
    "              0]),  # State vector\n",
    "        \"P\": 1000 * np.eye(4),  # Initial uncertainty, a random high number\n",
    "        \"F\":  np.array([[1, 1, 0, 0],  # x_pos\n",
    "                        [0, 1, 0, 0],  # x_vel\n",
    "                        [0, 0, 1, 1], # y_pos\n",
    "                        [0, 0, 0, 1]]),  # y_vel # Transition matrix\n",
    "        \"u\": np.zeros(4),  # External motion\n",
    "        \"H\": np.array([[1, 0, 0, 0],  # Observe x position\n",
    "                       [0, 0, 1, 0]]),  # Observe y position\n",
    "        \"R\": 10 * np.eye(2),  # Measurement uncertainty\n",
    "        \"I\": np.eye(4)  # Identity matrix\n",
    "    }\n",
    "    return kalman\n",
    "def update(kalman, Z):\n",
    "    x, P, H, R, I = kalman[\"x\"], kalman[\"P\"], kalman[\"H\"], kalman[\"R\"], kalman[\"I\"]\n",
    "    \n",
    "    # Measurement residual y\n",
    "    y = Z - np.dot(H, x)\n",
    "    \n",
    "    # Residual covariance S\n",
    "    S = np.dot(H, np.dot(P, H.T)) + R\n",
    "    \n",
    "    # Kalman gain K\n",
    "    K = np.dot(P, np.dot(H.T, np.linalg.inv(S)))\n",
    "\n",
    "    # Update state estimate x\n",
    "    x = x + np.dot(K, y)\n",
    "    \n",
    "    # Update uncertainty P\n",
    "    P = np.dot(I - np.dot(K, H), P)\n",
    "    \n",
    "    kalman[\"x\"], kalman[\"P\"] = x, P\n",
    "    return kalman\n",
    "\n",
    "def predict(kalman):\n",
    "    x, P, F, u = kalman[\"x\"], kalman[\"P\"], kalman[\"F\"], kalman[\"u\"]\n",
    "    \n",
    "    Q = np.eye(4) * 0.1  # small noise\n",
    "\n",
    "    # Predict state x\n",
    "    x = np.dot(F, x) + u\n",
    "    \n",
    "    # Predict uncertainty P\n",
    "    P = np.dot(F, np.dot(P, F.T)) + Q\n",
    "    \n",
    "    kalman[\"x\"], kalman[\"P\"] = x, P\n",
    "    return kalman\n",
    "\n",
    "def new_kalman(track_id, tracked_predictions):\n",
    "    if track_id not in tracked_predictions:\n",
    "            kalman = initialize_kalman(x_center, y_center)\n",
    "            tracked_predictions[track_id] = {\"kalman\": kalman, \"width\": x2 - x1, \"height\": y2 - y1, \"occlusion_rate\":occlusion_rate,}\n",
    "    return tracked_predictions\n",
    "\n",
    "def create_outputs(outputs,occlusion_rate,frame_path,label,track_id,x1,y1,x2,y2,x_center,y_center):\n",
    "    if occlusion_rate == 100:\n",
    "            outputs.append((frame_path.name,label,track_id, x1, y1, x2, y2, x_center, y_center, 3)) # not visible at all\n",
    "    if occlusion_rate<100 and occlusion_rate >= 50:\n",
    "            outputs.append((frame_path.name,label,track_id, x1, y1, x2, y2, x_center, y_center, 2)) # partially visible\n",
    "    if occlusion_rate<50 and occlusion_rate > 0:\n",
    "            outputs.append((frame_path.name,label,track_id, x1, y1, x2, y2, x_center, y_center, 1)) # mostly visible\n",
    "    else:\n",
    "            outputs.append((frame_path.name,label,track_id, x1, y1, x2, y2, x_center, y_center, 0)) # totally visible\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142.5\n",
      "142.5\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved as video2\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "# Folder containing the .png images\n",
    "image_folder = \"outputs2\"  # Replace with the path to your folder\n",
    "output_video_path = \"video2\"  # Output video file name\n",
    "\n",
    "# Video properties\n",
    "frame_rate = 0.01  # Frames per second\n",
    "\n",
    "# Get all .png files from the folder and sort them\n",
    "images = [img for img in os.listdir(image_folder) if img.endswith(\".png\")]\n",
    "images = natsorted(images)  # Sort files naturally (e.g., 1, 2, 10 instead of 1, 10, 2)\n",
    "\n",
    "if not images:\n",
    "    print(\"No .png images found in the folder.\")\n",
    "    exit()\n",
    "\n",
    "# Read the first image to get dimensions\n",
    "first_image_path = os.path.join(image_folder, images[0])\n",
    "frame = cv2.imread(first_image_path)\n",
    "height, width, layers = frame.shape\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Codec for .avi format\n",
    "video = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (width, height))\n",
    "\n",
    "# Loop through images and write them to the video\n",
    "for image in images:\n",
    "    image_path = os.path.join(image_folder, image)\n",
    "    frame = cv2.imread(image_path)\n",
    "    video.write(frame)\n",
    "\n",
    "    # Display the frame (optional)\n",
    "    cv2.imshow('Video Preview', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to quit preview\n",
    "        break\n",
    "\n",
    "# Release everything\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Video saved as {output_video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetector:\n",
    "    def __init__(self, device, weights, source_1, source_2, image_size, save_path, sort_max_age=10, sort_min_hits=3, sort_iou_thresh=0.3):\n",
    "        self.device = select_device(device)\n",
    "        #self.model = DetectMultiBackend(weights, self.device)\n",
    "        #self.model.names = dict(list(self.model.names.items())[:2] + list(self.model.names.items())[4:])\n",
    "        #self.stride, self.names, self.pt = self.model.stride, self.model.names, self.model.pt\n",
    "        self.imgsz = image_size\n",
    "        #self.model.warmup(imgsz=(1, 3, *self.imgsz))\n",
    "        self.dt = Profile(device=self.device)\n",
    "        self.source_1 = Path(source_1)\n",
    "        self.source_2 = Path(source_2)\n",
    "        self.files_1 = [f for f in self.source_1.glob('*') if f.suffix.lower() in ['.jpg', '.jpeg', '.png']]\n",
    "        self.files_2 = [f for f in self.source_2.glob('*') if f.suffix.lower() in ['.jpg', '.jpeg', '.png']]\n",
    "        \n",
    "        self.save_path = save_path\n",
    "        if not os.path.exists(self.save_path):\n",
    "            os.makedirs(self.save_path)\n",
    "        \n",
    "        # Initialize SORT Tracker\n",
    "        self.sort_tracker = Sort(max_age=sort_max_age, min_hits=sort_min_hits, iou_threshold=sort_iou_thresh)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for file_1, file_2 in zip_longest(self.files_1, self.files_2, fillvalue=None):\n",
    "            img_1 = cv2.imread(str(file_1))\n",
    "            img_2 = cv2.imread(str(file_2))\n",
    "            yield (img_1, img_2)\n",
    "    \n",
    "    def draw_boxes(self, img, bbox, identities=None, categories=None, names=None, offset=(0, 0)):\n",
    "        for i, box in enumerate(bbox):\n",
    "            x1, y1, x2, y2 = [int(i) for i in box]\n",
    "            x1 += offset[0]\n",
    "            x2 += offset[0]\n",
    "            y1 += offset[1]\n",
    "            y2 += offset[1]\n",
    "            id = int(identities[i]) if identities is not None else 0\n",
    "            data = (int((box[0] + box[2]) / 2), int((box[1] + box[3]) / 2))\n",
    "            label = f\"{id} {names[int(categories[i])]}\" if categories is not None else str(id)\n",
    "\n",
    "            color = self.compute_color_for_labels(id)\n",
    "            (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.rectangle(img, (x1, y1 - 20), (x1 + w, y1), (255, 191, 0), -1)\n",
    "            cv2.putText(img, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, [255, 255, 255], 1)\n",
    "            cv2.circle(img, data, 3, color, -1)\n",
    "\n",
    "        return img\n",
    "    \n",
    "    @smart_inference_mode()\n",
    "    def detect_object_2D(self, im, frame_num, save_images):\n",
    "        with self.dt:\n",
    "            image_original = im\n",
    "            im = letterbox(im, self.imgsz, stride=self.stride, auto=True)[0]\n",
    "            im = im.transpose((2, 0, 1))[::-1]\n",
    "            im = np.ascontiguousarray(im)\n",
    "            im = torch.from_numpy(im).to(self.device)\n",
    "            im = im.float().unsqueeze(0)\n",
    "            im /= 255\n",
    "            \n",
    "            pred = self.model(im)\n",
    "            pred = non_max_suppression(pred)\n",
    "            \n",
    "            for det in pred:\n",
    "                gn = torch.tensor(image_original.shape)[[1, 0, 1, 0]] #Normalization\n",
    "                det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], image_original.shape).round() # Rescale to original size\n",
    "                \n",
    "                dets_to_sort = np.empty((0, 6))\n",
    "                for *xyxy, conf, cls in reversed(det):\n",
    "                    if cls not in [0, 1, 4]:\n",
    "                        continue\n",
    "                    \n",
    "                    c = int(cls)\n",
    "                    coords = ((xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist())  # Normalized xywh\n",
    "                    #{conf:.2f}\", *coords\n",
    "                    # Agregar deteccin a dets_to_sort para clases 0 y 1\n",
    "                    x1, y1, x2, y2 = [int(coord) for coord in xyxy]\n",
    "                    dets_to_sort = np.vstack((dets_to_sort, np.array([x1, y1, x2, y2, conf, c])))\n",
    "                \n",
    "                # Update SORT tracker\n",
    "                tracked_dets = self.sort_tracker.update(dets_to_sort)\n",
    "\n",
    "                # Draw tracked bounding boxes\n",
    "                if len(tracked_dets) > 0:\n",
    "                    bbox_xyxy = tracked_dets[:, :4]\n",
    "                    identities = tracked_dets[:, 4]  # Object IDs\n",
    "                    categories = [0] * len(identities)  # Placeholder for categories if needed\n",
    "                    self.draw_boxes(image_original, bbox_xyxy, identities, categories, self.names)\n",
    "            \n",
    "            # Save or display the resulting frame\n",
    "            if save_images:\n",
    "                save_path = os.path.join(self.save_path, f\"{frame_num}.png\")\n",
    "                cv2.imwrite(save_path, image_original)\n",
    "\n",
    "    def compute_color_for_labels(self, label, palette=(2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)):\n",
    "        color = [int((p * (label ** 2 - label + 1)) % 255) for p in palette]\n",
    "        return tuple(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: cannot change to '/Users/dani/Desktop/MS_AutonomousSystems/Perception_for_Autonomous_systems/Final': No such file or directory\n",
      "YOLOv5  2024-11-21 Python-3.10.14 torch-2.2.2 CPU\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detector = ObjectDetector(device, weights, source_1, source_2, image_size, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Z ESTIMATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_to_object(left_center, right_center, im_left, im_right, focal_length = 707.0493, baseline = 0.06):\n",
    "\n",
    "    # Calculate disparity (horizontal pixel difference between the left and right image)\n",
    "    disparity = abs(left_center[0] - right_center[0])\n",
    "    \n",
    "    if disparity == 0:\n",
    "        return float('inf')\n",
    "    \n",
    "    Z = (focal_length * baseline) / disparity\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, images \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(detector):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_object_2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#detector.detect_object_2D(images[1], save_path=output, frame_num=i)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pfas/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 59\u001b[0m, in \u001b[0;36mObjectDetector.detect_object_2D\u001b[0;34m(self, im, frame_num, save_images)\u001b[0m\n\u001b[1;32m     56\u001b[0m im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     57\u001b[0m im \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[0;32m---> 59\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m pred \u001b[38;5;241m=\u001b[39m non_max_suppression(pred)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m det \u001b[38;5;129;01min\u001b[39;00m pred:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pfas/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pfas/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/MS_AutonomousSystems/Perception_for_Autonomous_systems/Final project/YoloV5_full/yolov5_object_track/models/common.py:688\u001b[0m, in \u001b[0;36mDetectMultiBackend.forward\u001b[0;34m(self, im, augment, visualize)\u001b[0m\n\u001b[1;32m    685\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize) \u001b[38;5;28;01mif\u001b[39;00m augment \u001b[38;5;129;01mor\u001b[39;00m visualize \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:  \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pfas/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pfas/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/MS_AutonomousSystems/Perception_for_Autonomous_systems/Final project/YoloV5_full/yolov5_object_track/models/yolo.py:270\u001b[0m, in \u001b[0;36mDetectionModel.forward\u001b[0;34m(self, x, augment, profile, visualize)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_augment(x)  \u001b[38;5;66;03m# augmented inference, None\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/MS_AutonomousSystems/Perception_for_Autonomous_systems/Final project/YoloV5_full/yolov5_object_track/models/yolo.py:169\u001b[0m, in \u001b[0;36mBaseModel._forward_once\u001b[0;34m(self, x, profile, visualize)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 169\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    170\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pfas/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pfas/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/MS_AutonomousSystems/Perception_for_Autonomous_systems/Final project/YoloV5_full/yolov5_object_track/models/common.py:91\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     90\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Applies a fused convolution and activation function to the input tensor `x`.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pfas/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pfas/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pfas/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pfas/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, images in enumerate(detector):\n",
    "    detector.detect_object_2D(images[0], frame_num=i, save_images=True)\n",
    "    #detector.detect_object_2D(images[1], save_path=output, frame_num=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Video from frames -><h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved as output_video.mp4\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths\n",
    "data_folder = \"./outputs\"  # Folder containing the images\n",
    "output_video = \"output_video.mp4\"  # Output video file\n",
    "\n",
    "# Video settings\n",
    "frame_rate = 30  # Frames per second\n",
    "frame_size = None\n",
    "\n",
    "# Collect all image paths, sorted by name\n",
    "image_paths = sorted(Path(data_folder).glob(\"*.png\"))  # Adjust the pattern to match your image format\n",
    "\n",
    "# Check if there are images to process\n",
    "if not image_paths:\n",
    "    raise ValueError(f\"No images found in {data_folder}\")\n",
    "\n",
    "# Read the first image to get the frame size\n",
    "first_image = cv2.imread(str(image_paths[0]))\n",
    "if first_image is None:\n",
    "    raise ValueError(\"Could not read the first image. Check the image path and format.\")\n",
    "frame_size = (first_image.shape[1], first_image.shape[0])  # (width, height)\n",
    "\n",
    "# Define the video writer with the correct codec\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for mp4\n",
    "out = cv2.VideoWriter(output_video, fourcc, frame_rate, frame_size)\n",
    "\n",
    "# Write each image to the video\n",
    "for image_path in image_paths:\n",
    "    frame = cv2.imread(str(image_path))\n",
    "    if frame is None:\n",
    "        print(f\"Warning: Could not read image {image_path}, skipping.\")\n",
    "        continue\n",
    "    resized_frame = cv2.resize(frame, frame_size)  # Ensure consistent size\n",
    "    out.write(resized_frame)\n",
    "\n",
    "# Release the video writer\n",
    "out.release()\n",
    "print(f\"Video saved as {output_video}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
